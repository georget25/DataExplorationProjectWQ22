---
title: "Data Exploration Project"
author: "George Thiss"
date: "2/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# loading packages
```{r}
library(tidyverse)
library(purrr)
library(car)
library(fmtr)
library(jtools)
```


# reading in data files
```{r}
trends <- list.files(pattern = "trends_", full.names = TRUE) %>% 
  lapply(read_csv) %>% 
  bind_rows() 
  
scorecard <- read.csv("Most+Recent+Cohorts+(Scorecard+Elements).csv", header = TRUE)

id_name_link <- read.csv("id_name_link.csv", header = TRUE)
```


# linking together the data
```{r}
trends_and_id_name <- left_join(x = id_name_link, y = trends, by = "schname")

ourdata <- left_join(x = trends_and_id_name, y = scorecard, by = c("unitid" = "UNITID"))
```
# only bachelors and clean median earning
```{r}
ourdata <- ourdata %>%
  rename(median_earn = 'md_earn_wne_p10.REPORTED.EARNINGS') %>%
  filter(PREDDEG == 3) %>%
  filter(median_earn != 'NULL') %>%
  filter(median_earn != 'PrivacySuppressed') %>%
  mutate(median_earn = as.numeric(median_earn))
```
# creating a dataset with no repeat names
```{r}
repeat_names <- ourdata %>% 
  group_by(unitid, schname) %>%
  summarize(mean(index, na.rm = TRUE)) %>%
  group_by(schname) %>%
  mutate(n = n()) %>%
  filter(n != 1)
```
# removing repeats from the data
```{r}
ourdata <- ourdata[!(ourdata$schname %in% repeat_names$schname),]
```


# ANALYSIS

# grouping by monthorweek
```{r}
month_or_week <- ourdata %>%
  group_by(monthorweek) %>%
  summarize(n = n())
```
# remove NAs in month_or_week
```{r}
month_or_week_clean <- na.omit(month_or_week, c("monthorweek"))
```
# dates before sept 2015
```{r}
before_sept <- month_or_week_clean$monthorweek[1:129]
before_sept <- as.data.frame(before_sept)
```
# creating binary variable where 0 is when the date is before september 2015, and 1 when it is after
```{r}
ourdata$before_scorecard<- ifelse(ourdata$monthorweek %in% before_sept$before_sept, 0, 1)
ourdata <- ourdata %>%
  relocate(before_scorecard, .after = monthorweek)
```
# finding earnings
```{r warning=FALSE}
earnings <- ourdata %>%
  group_by(unitid, schname) %>%
  summarize(mean(median_earn)) %>%
  na.omit(earnings, c("median_earn"))
```
# earnings
```{r warning=FALSE}
mean_earnings <- mean(earnings$'mean(median_earn)')
stdev_earnings <- sd(earnings$'mean(median_earn)')

mean_earnings - stdev_earnings
mean_earnings + stdev_earnings
```
# creating binary variables for high and low earnings based off +-  1 standard deviation away from the mean
```{r}
colnames(earnings)[3] <- "avg_earnings"

earnings$high_income <- ifelse(earnings$'avg_earnings' >= 54116.86, 1, 0)
earnings$low_income <- ifelse(earnings$'avg_earnings' <= 30506.58, 1, 0)
```

# standardizing the data
```{r}
ourdata <- ourdata %>%
  group_by(unitid, schname) %>%
  filter(index != 'NA') %>%
  mutate(standardized_index = (index - mean(index)) / sd(index))

ourdata <- ourdata %>%
  relocate(standardized_index, .after = index)
```

# combining earnings and ourdata
```{r}
ourdata2 <- merge(earnings, ourdata, by = c("unitid","schname"))
```


# GRAPHING

# Finding intrest in high earning schools
```{r}
highEarningSchools <- subset(ourdata2, high_income == 1)

intrestOVT_high <- highEarningSchools %>%
  group_by(monthorweek) %>%
  summarise(mean(standardized_index, na.rm = TRUE))

ggplot(intrestOVT_high, aes(x=monthorweek, y=`mean(standardized_index, na.rm = TRUE)`)) + geom_point() + geom_vline(xintercept = "2015-09-06 - 2015-09-12")
```

# Finding intrest in low earning schools
```{r}
lowEarningSchools <- subset(ourdata2, low_income == 1)

intrestOVT_low <- lowEarningSchools %>%
  group_by(monthorweek) %>%
  summarise(mean(standardized_index, na.rm = TRUE))

ggplot(intrestOVT_low, aes(x=monthorweek, y=`mean(standardized_index, na.rm = TRUE)`)) + geom_point() + geom_vline(xintercept = "2015-09-06 - 2015-09-12")
```

# REGRESSION
```{r}
lowEarnReg <- lm(standardized_index ~ before_scorecard, data = lowEarningSchools)
highEarnReg <- lm(standardized_index ~ before_scorecard, data = highEarningSchools)
export_summs(lowEarnReg, highEarnReg)
```

# playing around
```{r}
mainregressionhigh <- lm(standardized_index ~ before_scorecard*high_income, data = ourdata2)
mainregressionlow <- lm(standardized_index ~ before_scorecard*low_income, data = ourdata2)
export_summs(mainregressionhigh, mainregressionlow)
```

# WRITE UP
  This data analysis project is all about answering the question “Among colleges that predominantly grant bachelor’s degrees, did the release of the Scorecard shift student interest to high-earnings colleges relative to low-earnings ones (as proxied by Google searches for keywords associated with those colleges)?” I used data from the college scoreboard, which was released on september 12th, 2015. The college scoreboard is an online tool created by the United States Department of Education to compare the costs of higher education institutions in the US. 
  To help us answer this question, Use Google trends data to help us measure the interest shift between low and high earning colleges. I want to see if there is any correlation between the release of this scorecard and the amount of searches for these colleges on Google. Knowing this data could help us better understand the decision making process for primarily people seeking bachelors degrees, and if people thus cared more about the amount of money made after college over school name or other factors. 
  Before starting our analysis, I had to organize the data. Since Google restricts you from downloading mass amounts of data at once, I had several different google trends files I had to work with. I joined together this data and saved it into “trends”. Then I loaded in the .csv file that contained the original scorecard data. Lastly Iloaded in the id_name_link file so I could match the IDs and school names. Ithen linked together the data into 2 main sets, trends and ourdata, which is the main data I was working with. 
  Afterwards, Istarted cleaning the data. First filtering out non bachelor degree colleges so I could narrow down our search, then I removed schools without earnings information since we want to look at how the earnings affected searches. Then the repeat schools had to be removed. Once these were done, our cleaning of the data was complete. 
After that I created binary variables for both before september 2015 and after 2015, as well as high and low earnings. For the before and after september, after researching I found that the scorecard was released on September 12, so I included all the data through september 12th instead of just september 1st. I also removed the NAs from the data. After dates came earnings, where I calculated earnings. I decided to find the mean earnings and then find 1 standard deviation away from it for my low and high income thresholds. That ended up being over around $54,000 for high earnings and under $30,500 for low earnings. I wanted to do just 1 standard deviation away since using these numbers, I found the top third and bottom third for college graduate earnings. Since we also have a large dataset, our data won’t be as skewed one way or another, so 1 standard deviation away seems appropriate. For the end of the analysis, I standardized the data by subtracting the mean and divide by the standard deviation. This in effect shows us the difference between the reported trend and the mean, so we can compare for all keywords fairly. 
  For the graphing portion I decided to plot the monthorweek of the trend, and the average standardized index for the trends. I separated for low and high income earners, and plotted both separately. I also included a line for the week of september 12 to see what happens during that week to the average standardized index. What I found was that generally it followed the pattern that the trends had been for the past years. It goes up then drops then goes up and then drops. One thing to note was that in both high and low income, the peaks slowly decreased and the valleys slowly decreased, showing that searches for high and low income colleges in general were decreasing slowly over time. Overall it was hard to draw much from the graphs, so an OLS regression had to be used. 
  I originally did a simple regression, just regressing standardized_index on before_scorecard. On the model for low earnings schools, there was a 0.18 decrease in the standardized interest index value. For high earning schools, there was a 0.12 decrease in the standardized interest index value. Both R squared values were 0. Both of these coefficients had p values that were below the alpha of .001, thus they were statistically significant. SInce the low earning schools had a slightly higher decrease in the standardized interest index value, the regression is suggesting that there is a slightly larger negative effect on low earning schools. That being said, since we are using the standardized interest values, it is measuring how many standard deviations away from the mean and thus is still rather small. 
  Afterwards I ran two more regressions using low income and high income as interaction terms for their respective regressions. This only affected high income, which then saw a 0.19 decrease in the standardized interest index value, whereas the low income interaction term was 0. Interestingly, when I added the interaction term of high income to my high income regression, it was the only one that returned an R squared value greater than 0. 
  Neither of my regressions had control variables since there were no apparently evident variables that would affect these within the data. 
  When looking at this data in the real world, the results suggest that releasing the scorecard brought both low and high earning school’s standardized interest indexes down. That being said, they are still rather small and more research is needed for us to determine if the scorecard did in fact significantly affect how users searched for schools, specifically based on the income of past graduates. We also are working with a limited amount of data, specifically after the scorecard was released. We have data starting in 2013, but the last date we have data for is 2016. Only having approximately 9 months after the effect went into place limits our analysis we can draw. Similarly, since the scorecard was brand new in 2015, it may have not been as widely known about so searches based off the scorecard may have been higher in later years. Doing this analysis again with updated data through 2022 could be really interesting to see since we would have more information after the treatment was introduced. 



